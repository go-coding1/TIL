# 수업내용

### v$sql과 v$sqlarea 차이

만약 `SCOTT> select * from emp;`를 날리면 `SCOTT> select * from SCOTT.emp;` 이 된다. 똑같이 ACE유저가 같은 쿼리를 날리면 `ACE> select * from ACE.emp;`가 된다. 각 sql은 세션값이 다르게 취급되어버리는데 v$sqlarea에서는 같은 sql로 취급한다.

### dbms_xplan.display함수 vs dbms_xplan.display_cursor함수
display_cursors는 실제 문장이 수행이 되고 그 결과가 메모리(Library Shared Pool 안에 남아있는 실행결과 하나하나를 cursor라고 부른다.)에 남아 있어서 그 결과(Cursor)를 보는 함수 이고, 이와 다르게 파싱이 된 결과물을 plan_table에 담가놓고 여러가지 예상되는 정보들을 알 수 있게 한다. 실제 쿼리가 실행되는 것은 아니여서 db서버에 부담을줄일 수 있다.

### dbms_stats.gather_table_stats를 와 display_cursor()의 이해
[display_cursor예제](https://sites.google.com/site/ukja/books-cbo-in-depth/chapter2/02-dbms-xplan-display-cursor-sql)

아래 쿼리와 같이 `gather_table_stats`를 사용하고 파라미터로 `no_invalidate => false`를 주면 기존의 정보를 다 사용할 수 없게 된다. 테이블에 대해 stats를 다시 수집하게 하면 해당 테이블에 기존에 쌓여있던 stats를 새로 싹다 다시 모으라는 뜻이다. 그래서 기존의 stats를 사용하지 못한다.
```sql
exec dbms_stats.gather_table_stats(user, 't_plan', -cascade=>true, no_invalidate=>false); --해당 테이블에 대한 OPTIMIZER STATS를 수집해라 하는 뜻
```
대신 파라미터로 `no_invalidate => true`를 주면 기존의 정보를 활용하면서 새로 쌓이는 정보를 적절히 바꿔라 라는 뜻이다. 

`dbms_xplan.display()`는 `gather_table_stat()`가 수집한 정보를 바탕으로 예상 실행 계획을 짜는데 만약에 테이블에 새로 10000건의 정보를 insert한 후 실행 계획을 보면 10000건이 없다고 생각하고 예상 실행 계획을 짠다. 그렇게 되면 `dbms_xplan.display_cursor()`를 사용해서 `E-Rows`와 `A-Rows`의 개숫가 많이 차이 난다. 그래서 테이블에 변화가 생기면 `gather_table_stat()`를 사용해주어야 최근 테이블 stats를 얻어 정확한 실행 계획을 짤 수 있고 좀더 정확한 performance 측정이 가능하다.
```sql
-- typical format
select /*+ gather_plan_statistics */
    count(*)
from t_plan
where c1 = 'Few';

select * from
table(dbms_xplan.display_cursor(null, null, 'typical'));
```

### Table Statistics
Data Dictionary 안에 Table Statistics가 들어 있고 이 정보들을 Optimizer가 사용해 Plan Generator를 한다.

Execution Plan 에 나오는 Rows Bytes Costs는 누적값을 나타낸 것이다.
- Cost : 이 머신에선 Single Block I/O를 부른 예측 횟수이다. 시간을 Single Block IO 횟수로 환산한 것이다.

### Selectivity 및 Cardinality 이해
아래 내용은 Optimizer에서 사용하는 Data Dictionary의 Table Statistics이다.
```text
유일값수 는 종류의 갯수로 생각하면 된다.
emp.num_rows       : 1000
sales.num_rows     : 100000000

emp.empno 유일값수 : 1000
emp.job   유일값수 : 100 
emp.sal   최소값  : 0 
          최고값   : 3000
sales.sale_date 유일값수 : 10000
위 내용대로 data dictionaray에 값이 들어 있었다.
```
그후 아래 쿼리 실행한다.
```sql
  select *                             Selectivity(선택도)                Cardinality 
    from emp e, sales s         
   where e.empno = 7788            -- 1/1000  = 0.001            * 테이블의 num_rows = 1
     and e.job = 'MANAGER'         -- 1/100   = 0.01             * 테이블의 num_rows = 10
     and e.sal >= 1500             -- (3000-1500)/(3000-0) = 0.5 * 테이블의 num_rows = 100
     and s.sale_date = '20211012'  -- 1/10000 = 0.0001           * 테이블의 num_rows = 10000
     and ....
```
모든 조건에 index가 있다고 했을 때 Selectivity 값이 작은것을 선택(access조건)해야 효율이 좋다.

근데 선택도만 놓고 보면 `s.sale_date`가 제일 좋아서 `sales`테이블를 access하는 경우가 생길 수 있다. 근데 `sales` 테이블은 애초에 row의 수가 너무 많기 때문에 오히려 손해다. 결국 **Selectivity는 선택 비율이기 때문에 절대적인 구분기준이 될 수 없다.** Cardinality는 = Selectivity * 데이터 건수 로 계산 할 수 있다. 이 값을 통해서 예측 row 숫자를 계산할 수 있는데 이 숫자들이
```text
--------------------------------------------------------------------------------
| Id  | Operation         | Name       | Rows  | Bytes | Cost (%CPU)| Time     |
--------------------------------------------------------------------------------
|   0 | SELECT STATEMENT  |            |       |       |     1 (100)|          |
|   1 |  SORT AGGREGATE   |            |     1 |     6 |            |          |
|*  2 |   INDEX RANGE SCAN| T_PLAN_IDX |     1 |     6 |     1   (0)| 00:00:01 |
--------------------------------------------------------------------------------
```
에 있는 `ROWS`의 값이된다. **하지만 이건 예측 ROW라는 것을 생각해야 한다.**

조인은 데이터의 사이즈가 작은 테이블 부터 시작하는게 좋기 때문에 emp테이블부터 먼저 조인을 하는 것이 좋다.

만약에 SQL이 아래와 같으면
```sql
  select *                             Selectivity(선택도)                Cardinality 
    from emp e, sales s         
   where e.empno = 7788            -- 1/1000  = 0.001            * 테이블의 num_rows = 1
     and e.job = 'MANAGER'         -- 1/100   = 0.01             * 테이블의 num_rows = 10
     and e.sal >= 1500             -- (3000-1500)/(3000-0) = 0.5 * 테이블의 num_rows = 100
     and s.sale_date = '20211012'  -- 1/10000 = 0.0001           * 테이블의 num_rows = 10000
     and s.sale_id = '...'         -- 1/10000000 = 0.000001      * 테이블의 num_rows = 1
```
이 되면 `s.sale_id`를 이용해서 access 할 수도 있댜. 결국 모든 판단은 Optimizer가 Data Dictionary에 있는 Table Statistics를 보고 판단한다.

근데 이 Table Statistics를 최신으로 유지하는 것은 매우 비용이 많이 걸리고 시간도 많이 걸린다. 그래서 새벽에 자주 사용하는 테이블에 대해서만 조사를 하게 된다. 근데 가끔씩 조사를 잘못하는 경우가 있어서 Oracle에서 자동 또는 수동으로 정보를 저장해놓는다. 이 내용을 스크립트로 짜 놓을 수도 있다.(Trigger 이용)

Table Statistics 조작 한 예시
```sql

create table bricks (
  brick_id         integer not null primary key,
  colour_rgb_value varchar2(10) not null,
  shape            varchar2(10) not null,
  weight           integer not null
);

create table colours (
  colour_rgb_value varchar2(10) not null,
  colour_name      varchar2(10) not null
);

insert into colours values ( 'FF0000', 'red' );
insert into colours values ( '00FF00', 'green' );
insert into colours values ( '0000FF', 'blue' );

insert into bricks
  select rownum,
         case mod ( level, 3 )
           when 0 then 'FF0000'
           when 1 then '00FF00'
           when 2 then '0000FF'
         end,
         case mod ( level, 3 )
           when 0 then 'cylinder'
           when 1 then 'cube'
           when 2 then 'pyramid'
         end,
         floor ( 100 / rownum )
  from   dual
  connect by level <= 100;
  
insert into bricks
  select rownum + 1000,
         case mod ( level, 3 )
           when 0 then 'FF0000'
           when 1 then '00FF00'
           when 2 then '0000FF'
         end,
         case mod ( level, 3 )
           when 0 then 'cylinder'
           when 1 then 'cube'
           when 2 then 'pyramid'
         end,
         floor ( 200 / rownum )
  from   dual
  connect by level <= 200;
  
  
declare
  stats dbms_stats.statrec;
  distcnt  number; 
  density  number;
  nullcnt  number; 
  avgclen  number;
begin

  dbms_stats.gather_table_stats ( null, 'colours' );
  dbms_stats.gather_table_stats ( null, 'bricks' );
  dbms_stats.set_table_stats ( null, 'bricks', numrows => 30 );
  dbms_stats.set_table_stats ( null, 'colours', numrows => 3000 );
  dbms_stats.get_column_stats ( null, 'colours', 'colour_rgb_value', 
    distcnt => distcnt, 
    density => density,
    nullcnt => nullcnt, 
    avgclen => avgclen,
    srec => stats
  );
  stats.minval := utl_raw.cast_to_raw ( '0000FF' );
  stats.maxval := utl_raw.cast_to_raw ( 'FF0000' );
  dbms_stats.set_column_stats ( null, 'colours', 'colour_rgb_value', distcnt => 10, srec => stats );
  dbms_stats.set_column_stats ( null, 'bricks', 'colour_rgb_value', distcnt => 10, srec => stats );

end; -- Table Statistics 조작.

select table_name, num_rows, blocks
from dba_tables
where table_name in ('BRICKS', 'COLOURS');

select ut.table_name, ut.num_rows, 
       utcs.column_name, utcs.num_distinct, 
       case utc.data_type
         when 'VARCHAR2' then
           utl_raw.cast_to_varchar2 ( utcs.low_value ) 
        when 'NUMBER' then
           to_char ( utl_raw.cast_to_number ( utcs.low_value ) )
       end low_val, 
       case utc.data_type
         when 'VARCHAR2' then
           utl_raw.cast_to_varchar2 ( utcs.high_value ) 
         when 'NUMBER' then
           to_char ( utl_raw.cast_to_number ( utcs.high_value ) )
       end high_val
from   user_tables ut
join   user_tab_cols utc
on     ut.table_name = utc.table_name
join   user_tab_col_statistics utcs
on     ut.table_name = utcs.table_name
and    utc.column_name = utcs.column_name
where ut.table_name in ('BRICKS', 'COLOURS')
order  by ut.table_name, utcs.column_name;

select /*+ gather_plan_statistics */c.colour_name, count (*)
from   bricks b
join   colours c
on     c.colour_rgb_value = b.colour_rgb_value
group  by c.colour_name;

select sql_id
from v$sql
where sql_text like '% /*+ gather_plan_statistics */c.colour_name, count (*)%';
```
아래 결과
```text
--------------------------------------------------------------------------
| Id  | Operation                   | Name    | Starts | E-Rows | A-Rows |
--------------------------------------------------------------------------
|   0 | SELECT STATEMENT            |         |      1 |        |      3 |
|   1 |  HASH GROUP BY              |         |      1 |      3 |      3 |
|*  2 |   HASH JOIN                 |         |      1 |   9000 |    300 |
|   3 |    TABLE ACCESS STORAGE FULL| BRICKS  |      1 |     30 |    300 |
|   4 |    TABLE ACCESS STORAGE FULL| COLOURS |      1 |   3000 |      3 |
--------------------------------------------------------------------------
```
위 결과를 보면 조작한 숫자 때문에 실제 row가 다름 그래서 table statistics를 업데이트해야함, Optimize Stats?
```sql
exec dbms_stats.gather_table_stats ( null, 'colours' ) ;

select ut.table_name, ut.num_rows, utcs.column_name, utcs.num_distinct
from   user_tables ut
join   user_tab_col_statistics utcs
on     ut.table_name = utcs.table_name
where  ut.table_name = 'COLOURS';
```
기본적으로 오라클의 Optimizer는 테이블의 10퍼센트가 바뀌면 stats르 업데이트한다. 자동으로 통계를 수집하게 된다.
```sql
select dbms_stats.get_prefs ( 'STALE_PERCENT', null, 'colours' ) from dual;
exec dbms_stats.set_table_prefs ( null, 'colours', 'STALE_PERCENT', 1 ); -- 1퍼센트 바뀔때마다 stats를 업데이트함.
select dbms_stats.get_prefs ( 'STALE_PERCENT', null, 'colours' ) from dual;
```
table statistics의 정보를 강제로 업데이트 하고 싶으면 `exec dbms_stats.gather_table_stats ( null, 'colours', no_invalidate => false ) ;`를 사용해라.(`NO_INVALIDATE => FALSE`를 파라미터로 넣으면 즉시 반영) 근데 조심히 사용해야 한다.

근데 Table Statistics로는 Optimize Stats를 정확히 얻어 낼 수 없다. 왜냐하면 중복값은 Table Statistics에서 제거된 값이 나오기 때문에 정확하지 않다. 이걸 Data Skew가 일어날 수 있다고 한다.
---
### Data Modeling

### RelationShip Detail
관계의 종류 (10개)
- 1:1
    - 안필요 : 안필요
    - 필요   : 안필요
    - 필요  : 필요
- 1:N
    - 여러개 안필요 : 안필요
    - 여러개 필요 : 안필요
    - 여러개 안필요 : 필요
    - 여러개 필요 : 필요
- N:N
    - 여러개 안필요 : 여러개 안필요
    - 여러개 필요 : 여러개 안필요
    - 여러개 필요 : 여러개 필요

만약에 RelationShip이 속성을 가지면 그건 잘못된 Modeling을 한것이다.(엔티티만 속성을 가지고 있어야함)

정규화 규칙 등이 있다.

### Establishing a Relationship
Relationship을 찾는 순서
1. Existence : Relationship Matrix
2. Name      : Relationship Matrix 
3. Optionality
4. Degree : 1:1인지 N:N인지 판단하는 단계
5. Nontransferability : 값이 변할 수 있는지 판단하는 단계
6. Validate(검증) : 크게 소리내어 읽기
위 단계를 문장으로 만들 수 있다.
```text
Each entity1   must be       relationship name    one or more      entity2.
Each entity1   may be        relationship name    one or more      entity2.
Each entity1   must be       relationship name    one and only one entity2.
Each entity1   may be        relationship name    one and only one entity2.
               [optionality] [name]               [Degree]
```
이런식으로 관계를 지정하다 보면 총 10가지의 Relationship Types이 나오게 된다.

### 불필요한 관계
엔티티끼리 관계를 맺다 보면 있는 관계이지만 모델링에서는 불 필요한 관계가 생긴다. 그런 관계는 굳이 맺어 줄 필요는 없다.

### Relationship and Attribute
엔티티가 인스턴스가 될수도 있고 반대도 가능하다. Attribute가 엔티티가 될수도 있고 그 반대도 가능하다. 물론 **Relationship이 Attribute가 될수 있고 그 반대도 가능하다.** 

속성이 Relationship을 숨길수 있다. 이 말은 Relationship은 속성값으로 넣을 수 있다.

이런 관계들이 속성이 되고 여러 엔티티에 걸쳐서 속성이 된 관계들이 모여 하나의 엔티티를 이루는 것을 **Star Schema**라고 한다.

단, 논리적 모델링(ER Modeling)에서는 Fk가 속성인 경우는 없다. FK정의는 나중에 한다.

### Star Schema
이 곳에는 엄청나게 많은 코드들이 쌓이게 되는데 보통 `~~별 ~~별 ~~별 ~~별 의 합`이런 집계형식의 테이블이 된다.

### M:M Relationship(RDM_vol_1_English.pdf 127)
정규화 때는 M:M관계를 이어주는 테이블을 만들어줘야 하지만 ER Modeling에서는 새로운 엔티티를 만들어 그 엔티티와의 관계를 만들어 M:M을 1:N, 1:N으로 만들어줘야 한다.(130p) 새로운 엔티티에는 Artificial Attribute가 생겨 새로운 PK를 구성할 수도 있다.
 해결 순서(134p)
1. Create new intersection entity
2. Create two M:1 relationships, derive optionality
3. Remove M:M relationship

### Advanced Modeling
  - Normalize the Data Model

  - Resolve M:M Relationships = 교차엔티티 추가 + Relationship

  - Model Subtypes

  - Model Exclusive Relationships

  - Model Hierachical Data

  - Model Recursive Relationships

  - Model Role Relationships

  - Model Data over Time  >> cf.Flashback Data Archive + Flashback Query

  - Model Complex relationships

> 우리가 Modeling배워서 할 수 있어야 하는 것 
> 1. Modeling을 보고 읽을 수 있어야함
> 2. UID의 유무
> 3. M:M을 다른 엔티티를 추가해서 수정 할줄 알아야함

### Constraints
NN, Unique PK 설정하는 것이 아니라, 논리 모델의 Attribute에 설정하는 비즈니스 룰을 말하는 것이다.

UID는 종류가 다양하다.
- Single UID
- Multiple UID
- Composed UID

### 잘못된 UID
- 서로 maybe인데 UID인 경우
- 1:N관계에서 1쪽에 UID가 붙는 경우
- 3개의 엔티티끼리 UID가 순환이 되는경우

### 정상 UID
- 1:1 일때 Mandatory쪽에 UID 붙을 수 있음
- Mandatory인 1:N관계에 N쪽에 UID 붙을 수 있음
- N쪽에 서브타입이 여러개 인 경우 N쪽에 UID 붙을 수 있음, 또는 서브 타입 값이 있는 애들을 분리해서 테이블을 서브타입 갯수대로 테이블을 나눔

### Arcs
계좌엔티티가 있고 개인고객과 법인고객이 있다. 계좌는 각 고객의 PK를 가져야하는데 논리적으로 개인고객과 법인고객이 한번에 존재할 수 없다. 그래서 이런 관계를 배타적으로 처리해야하는데 그것을 Arcs라고 한다. 약간 서브타입처리와 비슷한 느낌이 있다. 서브타입때는 하나의 테이블로 두고 구분자를 놓거나 서브타입 갯수만큼 테이블을 따로 만들었었다.

Arcs에서는 2가지 방법이 있다.(RDM_vol2_English.pdf 51p)
1. Explicit Arc Design : 각 엔티티의 PK를 하나씩 속성값으로 만듦(null 이 되는 값이 생김) 관계형에서는 null 가능하고 하나의 값이 있으면 다른 곳에 값이 들어 갈 수 없는 제약을 걸어줘야한다. 장점은 FK제약은 걸어 무결성을 보장할 수 있다. 단점은 새로 엔티티가 Arcs되면 제약을 새로 다 넣어줘야 한다.
2. Generic Arcs Design : 하나의 속성에 PK들을 넣음(대신 데이터 타입이 똑같음). 징잠은 새로운 엔티티가 Arcs되어도 변하는게 없다. 단점은 FK제약을 걸수 없어서 FK제약 로직을 따로 구현해줘야한다.(UI적으로 처리하거나 비즈니스 적으로 처리해야함)

### SubType  to Arcs
서브타입이 포함된 슈퍼엔티티를 Arc타입으로 바꿀 수 있다. 서브타입을 슈퍼엔티티의 바깥으로 빼고 슈퍼엔티티의 pk를 빼낸 엔티티의 pk 값으로 넣어준다. 그 상태에서 각 빼낸 엔티티끼리의 관계를 Arcs 로 만들면 결과는 똑같다.
